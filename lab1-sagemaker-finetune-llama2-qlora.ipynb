{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75779f7f-682a-4fc3-b78f-5206fab818b5",
   "metadata": {},
   "source": [
    "# Finetune an LLM on Amazon SageMaker\n",
    "In this notebook, we are going to focus on 3 topics:\n",
    "\n",
    "1. Process a public available dataset for LLM training/finetuning \n",
    "2. Finetune an LLM using QLoRA, an efficient finetuning technique that matches the performance of full-precision fine-tuning approaches.\n",
    "3. Deploy the finetuned LLM for inference using SageMaker.\n",
    "\n",
    "For preprocessing the dataset, we use a SageMaker Processing job to help provide the compute resources required to complete the processing steps.\n",
    "\n",
    "For model finetuning, we'll be using a SageMaker Training job to automatically spins up compute resources, execute the model training steps, and shutdown the resources automatically when the job is complete. \n",
    "\n",
    "To deploy the finetuned model, we'll be using the SageMaker Python SDK to deploy the model into SageMaker for a fully managed HTTPS endpoint in a single command.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ee4f-d395-42d1-ae39-838e1d9854c7",
   "metadata": {},
   "source": [
    "First, we need to install the dependencies needed to run the notebook end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433f40d-5dbb-41b6-82b4-bea95d81fe09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 datasets pygments -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8845e-6810-4872-861e-27758fc31285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sagemaker.experiments.run import Run\n",
    "import uuid\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f1dc4-2b14-44d2-ac48-526f9e9fa74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaa9e7-7aef-4f1f-8e8e-7cf6753a34e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff28fed-c466-4e1e-baae-788e9d59988a",
   "metadata": {},
   "source": [
    "Define the variables to be used for the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b1461-d967-42c8-9f17-19fa5f8c951d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"base_model_pkg_group_name\" not in locals():\n",
    "    base_model_pkg_group_name = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54553e0f-ea37-42ce-9a5c-4b71bf4b944b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_id = uuid.uuid4().hex[:5] # this is the random-id assigned for each run. \n",
    "training_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/train\"\n",
    "validation_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/eval\"\n",
    "model_output_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/model\"\n",
    "model_eval_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/modeleval\"\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "hf_dataset_name = \"hotpot_qa\"\n",
    "\n",
    "print(f\"training_dataset_s3_loc: {training_dataset_s3_loc}\")\n",
    "print(f\"validation_dataset_s3_loc: {validation_dataset_s3_loc}\")\n",
    "print(f\"model artifact S3 location: {model_output_s3_loc}\")\n",
    "print(f\"model evaluation output S3 location: {model_eval_s3_loc}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"base model package group name: {base_model_pkg_group_name}\")\n",
    "print(f\"Huggingfae dataset name: {hf_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad14044-95a2-4d01-b182-1503bb449d00",
   "metadata": {},
   "source": [
    "# Proprocessing Data\n",
    "In our workshop, we'll build a generative AI chatbot application which requires the LLM the ability to understand instructions, and to provide accurate answer based on user query in natural language. \n",
    "For this reason, we choose an open source Llama2 base model [NousResearch-Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) which has been instruction tuned. We will finetune this model using good quality Q&A dataset. \n",
    "\n",
    "For our lab, we'll use a public dataset called [hotpotQA](https://hotpotqa.github.io/) as the data source. Here's a short summary of the dataset: \n",
    "\n",
    "HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. It is collected by a team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal.\n",
    "\n",
    "## SageMaker Processing\n",
    "\n",
    "To analyze data and evaluate machine learning models on Amazon SageMaker, we use a Amazon SageMaker Processing job. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "Here's a diagram that depicts how SageMaker Processing work:\n",
    "\n",
    "![sagemaker-processing](images/sagemaker-processing-diagram.png)\n",
    "\n",
    "In particular, we'll leverage a python script which contains the required code to handle the dataset. The script is executed in a Sagemaker processing job to automate the task end to end. The processing script can be shown in the following, and accessible in [src/preprocess/preprocess.py](src/preprocess/preprocess.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ae9c6-abe0-4fa1-bed7-a786badbf8b4",
   "metadata": {},
   "source": [
    "In the next cell, we'll process the data by running the script above as a SageMaker processing job. \n",
    "\n",
    "To launch a processing job, we use a Pytorch container by executing the `PytorchProcessor.run()` method. The `run()` method supports passing the arguments to the script.\n",
    "\n",
    "You can optionally provide input data in run() method to provide an input dataset on S3 bucket. By default, SageMaker processing job will download the data from the specified S3 location into local path inside the processing container in `/opt/ml/processing/input` directory.\n",
    "\n",
    "You could also provide an S3 location for the output data via the run() method by configuring an `ProcessingOutput` object. If not provided, SageMaker processing job defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/`. \n",
    "\n",
    "Following code shows the python script to be used for the processing job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349acdf0-84bc-4b07-b37d-873be055c90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize src/preprocess/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da8952-740c-4d8c-baf2-cfb63f9bc5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceProcessor\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "torch_processor = PyTorchProcessor(\n",
    "    framework_version='2.0',\n",
    "    role=get_execution_role(),\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    # instance_type='local', # uncomment for local mode\n",
    "    instance_count=1,\n",
    "    base_job_name='frameworkprocessor-PT',\n",
    "    py_version=\"py310\",\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb891a82-b01f-42b2-ab11-b0c5490192b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    source_dir=\"src/preprocess\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\",\n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=training_dataset_s3_loc),\n",
    "        ProcessingOutput(output_name=\"eval_data\",\n",
    "                         source=\"/opt/ml/processing/eval\",\n",
    "                         destination=validation_dataset_s3_loc),\n",
    "\n",
    "    ],\n",
    "    arguments=[\"--train-data-split\", \"1:50\",\n",
    "               \"--eval-data-split\", \"51:100\",\n",
    "               \"--hf-dataset-name\", hf_dataset_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f0c16-0580-4b54-b918-90ec46326a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune Llama2-7b model on Amazon SageMaker\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. \n",
    "QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. \n",
    "The TL;DR; of how QLoRA works is:\n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a train.py, which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code.\n",
    "\n",
    "Here's an animation that shows how how QLoRA works in general.\n",
    "\n",
    "![lora-animated](images/lora-animated.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed79eaf-b53d-4c78-b3a4-4fe04acf02e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize src/train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057ab0a-73e0-4d72-ad3f-d47c1f404f97",
   "metadata": {},
   "source": [
    "# Setting up Hyper Parameters for the fine tuning job\n",
    "The following section setup the hyperparameters required for finetuning a QLoRA model. \n",
    "\n",
    "For learn more about the hyperparameter setting for quantization and PEFT, please refer to [this](https://huggingface.co/docs/transformers/main_classes/quantization) and [this](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py) links.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6492f5-abf7-46c5-8912-48f048d82d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}-{rand_id}\"\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}-{rand_id}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'epochs': 2,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 8,                    # Batch size per GPU for training\n",
    "  'per_device_eval_batch_size' : 8,                    # Batch size per GPU for evaluation\n",
    "  'learning_rate' : 2e-4,                              # Initial learning rate (AdamW optimizer)\n",
    "  'optimizer' : \"paged_adamw_32bit\",                   # Optimizer to use\n",
    "  'logging_steps' : 5,                                 # Log every X updates steps\n",
    "  'lora_r': 64,                                        # LoRA attention dimension.\n",
    "  'lora_alpha' : 16,                                   # The alpha parameter for Lora scaling\n",
    "  'lora_dropout' : 0.1,                                # The dropout probability for Lora layers\n",
    "  'use_4bit' : True,                                   # Activate 4-bit precision base model loading\n",
    "  'bnb_4bit_compute_dtype' : \"float16\",                # Compute dtype for 4-bit base models\n",
    "  'bnb_4bit_quant_type' : \"nf4\",                       # Quantization type (fp4 or nf4)\n",
    "  'base_model_group_name' : base_model_pkg_group_name, # Base model registered in SageMaker Model Registry\n",
    "  'region': region,                                    # AWS region where the training is run\n",
    "  'model_eval_s3_loc' : model_eval_s3_loc              # S3 location for uploading the model evaluation metrics\n",
    "}\n",
    "\n",
    "print(f\"SageMaker experiment name: {experiments_name}\")\n",
    "print(f\"SageMaker experiment run name: {run_name}\")\n",
    "print(f\"SageMaker training job name: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a2194-f849-4083-818b-365f552e7387",
   "metadata": {},
   "source": [
    "## Run a SageMaker Training Job\n",
    "In this lab, we'll leverage SageMaker Training job to finetune a Llama2-7b model. The training job includes the following information:\n",
    "\n",
    "* The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n",
    "* The compute resources that you want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n",
    "* The URL of the S3 bucket where you want to store the output of the job.\n",
    "* The Amazon Elastic Container Registry path where the training code is stored. For more information.\n",
    "\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n",
    "\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace Estimator`. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In addition, the Estimator manages the infrastructure use. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`.\n",
    "\n",
    "After the training job, we'll use the estimator object to deploy the model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96027aea-4865-497a-9f21-3d9079b66b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with Run(\n",
    "    experiment_name=experiments_name,\n",
    "    run_name=run_name,\n",
    "    sagemaker_session=sess\n",
    ") as run:\n",
    "\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',         # train script\n",
    "        source_dir='src/train',         # directory which includes all the files needed for training\n",
    "        instance_type='ml.g5.2xlarge', # instances type used for the training job\n",
    "        # instance_type='local_gpu',      # use local \n",
    "        instance_count=1,               # the number of instances used for training\n",
    "        base_job_name=job_name,         # the name of the training job\n",
    "        role=get_execution_role(),      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "        volume_size=300,    # the size of the EBS volume in GB\n",
    "        transformers_version='4.28.1',    # the transformers version used in the training job\n",
    "        pytorch_version='2.0.0',          # the pytorch_version version used in the training job\n",
    "        py_version='py310',             # the python version used in the training job\n",
    "        hyperparameters= hyperparameters,\n",
    "        environment={ \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "        sagemaker_session=sess,         # specifies a sagemaker session object\n",
    "        output_path=model_output_s3_loc # s3 location for model artifact,\n",
    "    )\n",
    "    \n",
    "    # define a data input dictonary with our uploaded s3 uris\n",
    "    data = { 'training': training_dataset_s3_loc,\n",
    "             'validation': validation_dataset_s3_loc}\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    huggingface_estimator.fit(data, wait=True)\n",
    "    run.log_parameters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db322bf5-a425-43d5-a30b-479334777013",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker Experiment Integration\n",
    "### Overview\n",
    "Machine learning is an iterative process. You need to experiment with multiple combinations of data, algorithms, and parameters, all while observing the impact of incremental changes on model accuracy. Over time, this iterative experimentation can result in thousands of model training runs and model versions. This makes it hard to track the best performing models and their input configurations. It’s also difficult to compare active experiments with past experiments to identify opportunities for further incremental improvements. Use SageMaker Experiments to organize, view, analyze, and compare iterative ML experimentation to gain comparative insights and track your best performing models.\n",
    "\n",
    "### SageMaker Experiment\n",
    "Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you create, manage, analyze, and compare your machine learning experiments.\n",
    "SageMaker Experiments automatically tracks the inputs, parameters, configurations, and results of your iterations as runs. You can assign, group, and organize these runs into experiments. SageMaker Experiments is integrated with Amazon SageMaker Studio, providing a visual interface to browse your active and past experiments, compare runs on key performance metrics, and identify the best performing models. SageMaker Experiments tracks all of the steps and artifacts that went into creating a model, and you can quickly revisit the origins of a model when you are troubleshooting issues in production, or auditing your models for compliance verifications.\n",
    "\n",
    "As we saw in the training job configuration above, we enabled SageMaker Experiment to capture the relevant metrics to help us visualize the training and evalution metrics from the training run. To access these metrics in SageMaker experiment, you would navigate from the left panel:\n",
    "click on the Home icon ![home](images/home.png) -> Experiments -> name of the experiment (e.g. exp-nousresearch-llama-2-7b-chat-hf) -> run name.\n",
    "\n",
    "Here's are a screenshot of SageMaker Experiment that captures the metrics from a previous run:\n",
    "<div>\n",
    "<img src=\"images/experiment-metrics.png\" width=\"800\"/>\n",
    "<img src=\"images/exp-metrics-chart.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "For see more examples on SageMaker Experiment, please visit [amazon-sagemake-examples](https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-experiments) github reposistory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c9a15-e812-43f1-997e-1632ba3a5598",
   "metadata": {},
   "source": [
    "# Deploy the finetuned Llama2 model in SageMaker\n",
    "State-of-the-art deep learning models for applications such as natural language processing (NLP) are large, typically with tens or hundreds of billions of parameters. Larger models are often more accurate, which makes them attractive to machine learning practitioners. However, these models are often too large to fit on a single accelerator or GPU device, making it difficult to achieve low-latency inference. You can avoid this memory bottleneck by using model parallelism techniques to partition a model across multiple accelerators or GPUs.\n",
    "\n",
    "Amazon SageMaker includes specialized deep learning containers (DLCs), libraries, and tooling for model parallelism and large model inference (LMI). In the following sections, you can find resources to get started with LMI on SageMaker.\n",
    "\n",
    "With these DLCs you can use third party libraries such as [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Accelerate](https://huggingface.co/docs/accelerate), and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) to partition model parameters using model parallelism techniques to leverage the memory of multiple GPUs for inference.\n",
    "\n",
    "After the training job, we will deploy the QLoRA finetuned model into SageMaker for inference. In our example, we will also use a Large Model Inference(LMI) container provided by AWS using `DJL Serving` and `DeepSpeed`. Given the llama2-7b model size, this model could fit in a single `ml.g5.2xlarge` instance on AWS SageMaker.\n",
    "\n",
    "### Deep Java Library (DJL) \n",
    "Deep Java Library (DJL) Serving is a high performance universal stand-alone model serving solution powered by DJL. DJL Serving supports loading models trained with a variety of different frameworks. With the SageMaker Python SDK you can use DJL Serving to host large models using backends like DeepSpeed and HuggingFace Accelerate.\n",
    "\n",
    "For more information about using `DJL Serving` model server for hosting LLMs in SageMaker, please refer to the following:\n",
    "\n",
    "* [DeepSpeed and Accelerate](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-deepspeed-djl.html)\n",
    "* [FasterTransformer](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7604071-1db4-4376-90a4-3e398a9d1769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "llm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cf047-f104-46c8-bf8f-5132bfb083f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "# create HuggingFaceModel with the a DJI image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    image_uri=llm_image,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ffe1d-7e7c-4a08-899b-565331c36b93",
   "metadata": {},
   "source": [
    "Trigger a SageMaker deployment by invoking huggingface model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb32f0-2d61-4291-a79b-84fdf0fa2ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name_random_id = uuid.uuid4().hex[:5]\n",
    "endpoint_name = f\"llama2-7b-djl-deepspeed-{endpoint_name_random_id}\"\n",
    "\n",
    "print(f\"endpoint name: {endpoint_name}\")\n",
    "llm = huggingface_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, \n",
    "  endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c38b8-dfa2-44f6-9fae-f96bf35d3883",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the model\n",
    "In the following section, we'll run a test against the deployed endpoint. Here we format the \n",
    "prompt using the llama2 [standard prompt](https://huggingface.co/blog/llama2#how-to-prompt-llama-2).\n",
    "\n",
    "In the test data, we provide a system prompt along with a question and a few contextual information that might be relevant to the answer. Let's see how the model performs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f64c8-76f1-43ea-b61c-e38c12744f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>\n",
    "[INST] <<SYS>>\n",
    "{{system}}\n",
    "<</SYS>>\n",
    "\n",
    "### Question\n",
    "{{question}}\n",
    "\n",
    "### Context\n",
    "{{context}}[/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcdb7f-4dcc-42cd-9c93-3133bf19a403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"Given the following context, answer the question as accurately as possible:\"\n",
    "def build_llama2_prompt(message):\n",
    "    question = message['question']\n",
    "    context = message['context']\n",
    "    formatted_message = prompt_template.replace(\"{{system}}\", system_message)\n",
    "    formatted_message = formatted_message.replace(\"{{question}}\", question)\n",
    "    formatted_message = formatted_message.replace(\"{{context}}\", context)\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21659f11-b9ce-44ad-bfb1-20f0a4231d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = {}\n",
    "message['question'] = \"The Oberoi family is part of a hotel company that has a head office in what city?\"\n",
    "message['context'] = \"\"\"The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta. It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively. The hotel was opened in 2005.\n",
    "The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.\n",
    "The Oberoi Group is a hotel company with its head office in Delhi. Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\n",
    "The 289th Military Police Company was activated on 1 November 1994 and attached to Hotel Company, 3rd Infantry (The Old Guard), Fort Myer, Virginia. Hotel Company is the regiment\\'s specialty company.\\nThe Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel. The hotel is located at 209-215 East Barnard Street. The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920. The Glennwanis was built in brick in 1926. The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders. The wife of a local doctor won a naming contest with the name \"Glennwanis Hotel\", a suggestion combining \"Glennville\" and \"Kiwanis\".'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18857e2-4cbc-416f-aaf6-bf5834d47b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = build_llama2_prompt(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415fa153-d9c3-42d8-bea1-b737f486954e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a3e87-3556-4692-8a50-7296b90a2c9c",
   "metadata": {},
   "source": [
    "Run a prediction with inference configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e564c5-e405-4c1f-83c2-2cf73953a895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "  }\n",
    "\n",
    "output = llm.predict({\"text\":input, \"properties\" : params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e346701-2986-44d4-87ce-404b0d2102ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output['outputs'][0][\"generated_text\"][len(input):]) # automatically removed the bos_token and eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837618c-7676-4361-87cd-0e04b2385014",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d55e04-c5ad-4f7e-b1d8-be7a8369a8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f3b65-724d-4bea-8783-4eb3a1e5cb94",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "In this lab we saw how we can use a dataset from huggingface hub as the dataset, process the data for model funetuning step using a SageMaker Processing job. Then we saw how we could use a base model registered in SageMaker Model Registry to finetune a Llama2 model. We also saw how we could host the finetuned model with SageMaker Hosting service for inference.\n",
    "\n",
    "In the next lab, we'll focus on automating the entire LLM model training/finetuning pipeline using Amazon SageMaker Pipeline, and integrates with CICD pipeline to orchestrate model deployment process at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c18db-60f3-46b4-8d4b-ca80201bbb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
