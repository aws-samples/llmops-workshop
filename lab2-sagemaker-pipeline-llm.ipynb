{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bc5b75-77e9-436b-857b-f28a2e118e29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automate LLM training and deployment pipeline using SageMaker Pipelines\n",
    "\n",
    "Amazon SageMaker Pipelines offers machine learning (ML) application developers and operations engineers the ability to orchestrate SageMaker jobs and author reproducible ML pipelines. It also enables them to deploy custom-built models for inference in real-time with low latency, run offline inferences with Batch Transform, and track lineage of artifacts. They can institute sound operational practices in deploying and monitoring production workflows, deploying model artifacts, and tracking artifact lineage through a simple interface, adhering to safety and best practice paradigms for ML application development.\n",
    "\n",
    "In the previous lab, we saw how we could use SageMaker to simplify the data processing, finetuning an LLM and deploy the finetuned model and run inferences. In this lab, we'll use SageMaker Pipelines to help us automate the entire LLM training and deployment process using serverless and event driven architecture. \n",
    "\n",
    "Here's a high level architecture diagram for the LLMOps workflow:\n",
    "\n",
    "<img src=\"images/mlops-llm.drawio.png\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713c6ec-5435-4090-bb2b-d797b4563db4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook shows how to:\n",
    "\n",
    "* Define a set of Pipeline parameters that can be used to parametrize a SageMaker Pipeline.\n",
    "* Define a Processing step that performs feature engineering of a Huggingface dataset and split the dataset into train and evaluation data sets.\n",
    "* Define a Training step that finetunes a llama2-7b model on the preprocessed data set.\n",
    "* Define a Create Model step that creates a model from the model artifacts used in training.\n",
    "* Define a Register Model step that creates a model package from the estimator and model artifacts used to finetune the model.\n",
    "* Define and create a Pipeline definition in a DAG, with the defined parameters and steps.\n",
    "* Start a Pipeline execution and wait for execution to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44ff05a-95ba-4309-8fbd-68bb99811fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'sagemaker' --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e82187-e374-4db6-9349-d5a514fd62c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "region = pipeline_session.boto_region_name\n",
    "default_bucket = pipeline_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2cc819-68e1-4908-8f79-4c36300c8c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger, ParameterBoolean\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "processing_instance_count = 1\n",
    "training_instance_count = 1\n",
    "transform_instance_count = 1\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "rand_id = uuid.uuid4().hex[:5] # this is the random-id assigned for each run. \n",
    "training_dataset_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/train\"\n",
    "validation_dataset_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/eval\"\n",
    "model_output_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/model\"\n",
    "model_eval_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/modeleval\"\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "hf_dataset_name = \"hotpot_qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f28db46-c557-4335-9496-b6466eed4bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"T12H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b6822-1b5e-450c-9899-aa4a3c7fec76",
   "metadata": {},
   "source": [
    "# Define Parameters to parametize SageMaker Pipeline Executions\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "* ParameterInteger - represents an int Python type\n",
    "* ParameterFloat - represents a float Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow include:\n",
    "\n",
    "* processing_output_s3_location_param - processing job output S3 location. \n",
    "* model_id_param - huggingface model ID to indentify the base model\n",
    "* epochs_param - number of epochs to finetune the model\n",
    "* per_device_train_batch_size_param - training dataset batch size \n",
    "* per_device_eval_batch_size_param - evaluation dataset batch size  \n",
    "* learning_rate_param - QLoRA finetuning learning rate\n",
    "* optimizer_param - the model optimizer\n",
    "* logging_steps_param - number of steps for logging\n",
    "* lora_r_param - LoRA attention dimension\n",
    "* lora_alpha_param - Alpha parameter for LoRA scaling\n",
    "* lora_dropout_param - LoRA dropout rate\n",
    "* use_4bit_param - load base model in 4bit\n",
    "* bnb_4bit_compute_dtype_param - default compute type for quantization\n",
    "* bnb_4bit_quant_type_param - default quantization type\n",
    "* training_job_instance_type_param - training job instance type\n",
    "* model_output_s3_loc_param - model artifact output S3 location\n",
    "* training_dataset_s3_loc_param - S3 location for training dataset\n",
    "* eval_dataset_s3_loc_param - S3 location for evaluation dataset\n",
    "* training_dataset_split_param - training dataset split configuration\n",
    "* eval_dataset_split_param - evaluation dataset split configuration\n",
    "* base_model_group_name_param - base model package group name\n",
    "* region_param - AWS region name\n",
    "* model_eval_s3_loc_param - S3 location for model evalution metrics JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd50e7b-9c5a-474b-9d40-80a430209af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c86fa55d-a427-4f48-b2a2-c875669ad88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"base_model_pkg_group_name\" not in locals():\n",
    "    base_model_pkg_group_name = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861754e5-b4a3-4ccb-b468-0e6e5f28b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_output_s3_location_param = ParameterString(name=\"ProcessingOutputS3Location\", default_value=training_dataset_s3_loc)\n",
    "model_id_param = ParameterString(name=\"ModelId\", default_value=model_id)\n",
    "epochs_param = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "per_device_train_batch_size_param = ParameterInteger(name=\"PerDeviceTrainBatchSize\", default_value=8)\n",
    "per_device_eval_batch_size_param = ParameterInteger(name=\"PerDeviceEvalBatchSize\", default_value=8)\n",
    "learning_rate_param = ParameterFloat(name=\"LearningRate\", default_value=2e-4)\n",
    "optimizer_param = ParameterString(name=\"Optimizer\", default_value=\"paged_adamw_32bit\")\n",
    "logging_steps_param = ParameterInteger(name=\"LoggingSteps\", default_value=25)\n",
    "lora_r_param = ParameterInteger(name=\"LoraR\", default_value=64)\n",
    "lora_alpha_param = ParameterInteger(name=\"LoraAlpha\", default_value=16)\n",
    "lora_dropout_param = ParameterFloat(name=\"LoraDropout\", default_value=0.1)\n",
    "use_4bit_param = ParameterBoolean(name=\"Use4Bit\", default_value=True)\n",
    "bnb_4bit_compute_dtype_param = ParameterString(name=\"BnB4BitComputeType\", default_value=\"float16\")\n",
    "bnb_4bit_quant_type_param = ParameterString(name=\"BnB4BitQuantType\", default_value=\"nf4\")\n",
    "training_job_instance_type_param = ParameterString(name=\"TrainingJobInstanceType\", default_value=\"ml.g5.2xlarge\")\n",
    "processing_job_instance_type_param = ParameterString(name=\"ProcessingJobInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_output_s3_loc_param = ParameterString(name=\"ModelOutputS3LocParam\", default_value=model_output_s3_loc)\n",
    "training_dataset_s3_loc_param = ParameterString(name=\"TrainingDatasetS3LocParam\", default_value=training_dataset_s3_loc)\n",
    "eval_dataset_s3_loc_param = ParameterString(name=\"EvalDatasetS3LocParam\", default_value=validation_dataset_s3_loc)\n",
    "training_dataset_split_param = ParameterString(name=\"TrainingDatasetSplitParam\", default_value=\"1:50\")\n",
    "eval_dataset_split_param = ParameterString(name=\"EvalDatasetSplitParam\", default_value=\"51:100\")\n",
    "base_model_group_name_param = ParameterString(name=\"BaseModelRegistryGroupName\", default_value=base_model_pkg_group_name)\n",
    "region_param = ParameterString(name=\"RegionNameParam\", default_value=\"us-east-1\")\n",
    "model_eval_s3_loc_param = ParameterString(name=\"ModelEvalS3LocParam\", default_value=model_eval_s3_loc)\n",
    "hf_dataset_name_param = ParameterString(name=\"HFDataSetNameParam\", default_value=hf_dataset_name)\n",
    "base_model_package_group_name_param = ParameterString(name=\"BaseModelPkgGoupName\", default_value=base_model_pkg_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9941bc9-f69e-491d-9540-d67b68c04ebe",
   "metadata": {},
   "source": [
    "# Define a Processing Step\n",
    "A processing step is used for triggering a processing job for data processing. \n",
    "In this example, we are going to use a processing job to perform feature engineering on a public dataset available on Huggingface Hub. The output from the processing step will be stored in the specified S3 location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5d7538-faa6-4c97-8c7a-765214d382f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "torch_processor = PyTorchProcessor(\n",
    "    framework_version='2.0',\n",
    "    role=role,\n",
    "    instance_type=processing_job_instance_type_param,\n",
    "    instance_count=1,\n",
    "    base_job_name=f'frameworkprocessor-PT-{rand_id}',\n",
    "    py_version=\"py310\",\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86993ffe-66df-4f4e-bc6b-c5f0bf2bf35c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor_args = torch_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    source_dir=\"src/preprocess\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\",\n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=training_dataset_s3_loc_param),\n",
    "        ProcessingOutput(output_name=\"eval_data\",\n",
    "                         source=\"/opt/ml/processing/eval\",\n",
    "                         destination=eval_dataset_s3_loc_param),\n",
    "\n",
    "    ],\n",
    "    arguments=[\"--train-data-split\", training_dataset_split_param,\n",
    "               \"--eval-data-split\", eval_dataset_split_param,\n",
    "               \"--hf-dataset-name\", hf_dataset_name_param]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc503b7-9475-4fa9-8acb-114971102bec",
   "metadata": {},
   "source": [
    "Define a procesing step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e5677e-20e0-4ac9-baf0-c660805f9dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_process = ProcessingStep(name=\"LLMDataPreprocess\",\n",
    "                              step_args=processor_args,\n",
    "                              cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf07e59-a2ed-40fa-8dbf-69c94e5eec2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Step\n",
    "In this section, use define a training step to finetune a Llama2-7b model on the given dataset. \n",
    "Configure an Estimator for the HuggingFace and the input dataset. \n",
    "A typical training script loads data from the input channels, configures training with \n",
    "hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later.\n",
    "\n",
    "The model path where the models from training are saved is also specified.\n",
    "\n",
    "**Note:** the instance_type parameter may be used in multiple places in the pipeline. In this case, the instance_type is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "602bf0dd-a5be-4b2d-8cb7-abf4de3b70f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}-{rand_id}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id_param,                                # pre-trained model\n",
    "  'epochs': epochs_param,                                     # number of training epochs\n",
    "  'per_device_train_batch_size': per_device_train_batch_size_param,\n",
    "  'per_device_eval_batch_size' : per_device_eval_batch_size_param,\n",
    "  'learning_rate' : learning_rate_param,\n",
    "  'optimizer' : optimizer_param,  \n",
    "  'logging_steps' : logging_steps_param,\n",
    "  'lora_r': lora_r_param,\n",
    "  'lora_alpha' : lora_alpha_param,\n",
    "  'lora_dropout' : lora_dropout_param, \n",
    "  'use_4bit' : use_4bit_param,\n",
    "  'bnb_4bit_compute_dtype' : bnb_4bit_compute_dtype_param,\n",
    "  'bnb_4bit_quant_type' : bnb_4bit_quant_type_param,\n",
    "  'base_model_group_name' : base_model_group_name_param,\n",
    "  'region' : region_param,\n",
    "  'model_eval_s3_loc' : model_eval_s3_loc_param,\n",
    "  'run_experiment' : \"False\"\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',         # train script\n",
    "    source_dir='src/train',         # directory which includes all the files needed for training\n",
    "    instance_type=training_job_instance_type_param, # instances type used for the training job\n",
    "    instance_count=1,               # the number of instances used for training\n",
    "    base_job_name=job_name,         # the name of the training job\n",
    "    role=role,      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,    # the size of the EBS volume in GB\n",
    "    transformers_version='4.28.1',    # the transformers version used in the training job\n",
    "    pytorch_version='2.0.0',          # the pytorch_version version used in the training job\n",
    "    py_version='py310',             # the python version used in the training job\n",
    "    hyperparameters= hyperparameters,\n",
    "    environment={ \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    sagemaker_session=pipeline_session,         # specifies a sagemaker session object\n",
    "    output_path=model_output_s3_loc_param # s3 location for model artifact\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e3acdb1-7772-4372-ace1-2002b07da0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {'training': step_process.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "        'validation': step_process.properties.ProcessingOutputConfig.Outputs[\"eval_data\"].S3Output.S3Uri\n",
    "       }\n",
    "\n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}-{rand_id}\"\n",
    "# starting the train job with our uploaded datasets as input\n",
    "train_args = huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897109aa-bbab-4dac-bfa9-b1505233ea4a",
   "metadata": {},
   "source": [
    "Define the Training step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0850c8-c8c2-4701-97eb-6110b8b28518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Llama2Train\",\n",
    "    step_args=train_args,\n",
    ")\n",
    "step_train.add_depends_on([step_process])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c145be-b6a8-4e8d-8b56-5a1b803f1941",
   "metadata": {},
   "source": [
    "# Register Model\n",
    "SageMaker Model Registry supports the following features and functionality:\n",
    "\n",
    "* Catalog models for production.\n",
    "* Manage model versions. \n",
    "* Associate metadata, such as training metrics, with a model.\n",
    "* Manage the approval status of a model.\n",
    "* Deploy models to production.\n",
    "* Automate model deployment with CI/CD.\n",
    "\n",
    "In this workshop, we are going to register the finetuned LLama2 model as a model package using SageMaker Model Registry. \n",
    "\n",
    "A model package is an abstraction of reusable model artifacts that packages all ingredients required for inference. \n",
    "Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6cef850-7542-411f-8984-5b8b9d9db790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import json\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    image_uri=llm_image,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    name=f\"HuggingFaceModel-Llama2-7b-{rand_id}\",\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "create_step_args = huggingface_model.create(instance_type=inference_instance_type)\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    step_args=create_step_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80497d2b-5550-48be-b9fb-64d02a0ca3d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Metrics\n",
    "To capture the model training and evalution metrics from a SageMaker Training job, we use a `ModelMetrics` class. We captured the model evaluation metrics in a `evaluation.json`, stored in the specified S3 location. With that information, we create a `ModelMetrics` object to include the metrics. The object is used to register the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfe56123-405d-4c8e-a345-4b5afa42da50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "import os \n",
    "\n",
    "model_package_group_name = f\"NousResearch-Llama-2-7b-chat-hf-{rand_id}\"\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=os.path.join(model_eval_s3_loc, \"evaluation.json\"),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b8bbf81-2541-45e3-8176-c8e9fb6511c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "register_args = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\", \n",
    "        \"ml.g5.2xlarge\",\n",
    "        \"ml.g5.12xlarge\",\n",
    "    ],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    customer_metadata_properties = {\"training-image-uri\": huggingface_estimator.training_image_uri()},  #Store the training image url\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "step_register = ModelStep(name=\"RegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1269da1-c952-406f-b4eb-60ec2e60c3fe",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters and Steps \n",
    "In this section, we combine all the steps into a Pipeline so it can be executed.\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the data dependency DAG as steps for the execution to complete.\n",
    "* Steps must be unique to across the pipeline step list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd0c07f-04ad-4960-887e-0edcee2216f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_experiment_config import PipelineExperimentConfig\n",
    "\n",
    "pipeline_name = f\"Llama2FMOpsPipeline-{rand_id}\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_output_s3_location_param,\n",
    "        model_id_param,\n",
    "        epochs_param,\n",
    "        per_device_train_batch_size_param,\n",
    "        per_device_eval_batch_size_param,\n",
    "        learning_rate_param,\n",
    "        optimizer_param,\n",
    "        logging_steps_param,\n",
    "        lora_r_param,\n",
    "        lora_alpha_param,\n",
    "        lora_dropout_param,\n",
    "        use_4bit_param,\n",
    "        bnb_4bit_compute_dtype_param,\n",
    "        bnb_4bit_quant_type_param,\n",
    "        training_job_instance_type_param,\n",
    "        model_output_s3_loc_param,\n",
    "        training_dataset_s3_loc_param,\n",
    "        eval_dataset_s3_loc_param,\n",
    "        training_dataset_split_param,\n",
    "        eval_dataset_split_param,\n",
    "        base_model_group_name_param,\n",
    "        region_param,\n",
    "        model_eval_s3_loc_param,\n",
    "        hf_dataset_name_param,\n",
    "        processing_job_instance_type_param,\n",
    "        base_model_package_group_name_param\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model,step_register],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa337cc9-4f2f-42d4-a54f-44426cdb5741",
   "metadata": {},
   "source": [
    "# Examining the pipeline definition\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b88b1c2e-42b4-434e-96f5-359f9338365e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingOutputS3Location',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-2-349674815289/data/workshop-6e737/train'},\n",
       "  {'Name': 'ModelId',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'NousResearch/Llama-2-7b-chat-hf'},\n",
       "  {'Name': 'Epochs', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'PerDeviceTrainBatchSize', 'Type': 'Integer', 'DefaultValue': 8},\n",
       "  {'Name': 'PerDeviceEvalBatchSize', 'Type': 'Integer', 'DefaultValue': 8},\n",
       "  {'Name': 'LearningRate', 'Type': 'Float', 'DefaultValue': 0.0002},\n",
       "  {'Name': 'Optimizer', 'Type': 'String', 'DefaultValue': 'paged_adamw_32bit'},\n",
       "  {'Name': 'LoggingSteps', 'Type': 'Integer', 'DefaultValue': 25},\n",
       "  {'Name': 'LoraR', 'Type': 'Integer', 'DefaultValue': 64},\n",
       "  {'Name': 'LoraAlpha', 'Type': 'Integer', 'DefaultValue': 16},\n",
       "  {'Name': 'LoraDropout', 'Type': 'Float', 'DefaultValue': 0.1},\n",
       "  {'Name': 'Use4Bit', 'Type': 'Boolean', 'DefaultValue': True},\n",
       "  {'Name': 'BnB4BitComputeType', 'Type': 'String', 'DefaultValue': 'float16'},\n",
       "  {'Name': 'BnB4BitQuantType', 'Type': 'String', 'DefaultValue': 'nf4'},\n",
       "  {'Name': 'TrainingJobInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.g5.2xlarge'},\n",
       "  {'Name': 'ModelOutputS3LocParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-2-349674815289/data/workshop-6e737/model'},\n",
       "  {'Name': 'TrainingDatasetS3LocParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-2-349674815289/data/workshop-6e737/train'},\n",
       "  {'Name': 'EvalDatasetS3LocParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-2-349674815289/data/workshop-6e737/eval'},\n",
       "  {'Name': 'TrainingDatasetSplitParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': '1:50'},\n",
       "  {'Name': 'EvalDatasetSplitParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': '51:100'},\n",
       "  {'Name': 'BaseModelRegistryGroupName',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'None'},\n",
       "  {'Name': 'RegionNameParam', 'Type': 'String', 'DefaultValue': 'us-east-1'},\n",
       "  {'Name': 'ModelEvalS3LocParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-west-2-349674815289/data/workshop-6e737/modeleval'},\n",
       "  {'Name': 'HFDataSetNameParam',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'hotpot_qa'},\n",
       "  {'Name': 'ProcessingJobInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'BaseModelPkgGoupName', 'Type': 'String', 'DefaultValue': 'None'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'LLMDataPreprocess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingJobInstanceType'},\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.0-cpu-py310',\n",
       "     'ContainerArguments': ['--train-data-split',\n",
       "      {'Get': 'Parameters.TrainingDatasetSplitParam'},\n",
       "      '--eval-data-split',\n",
       "      {'Get': 'Parameters.EvalDatasetSplitParam'},\n",
       "      '--hf-dataset-name',\n",
       "      {'Get': 'Parameters.HFDataSetNameParam'}],\n",
       "     'ContainerEntrypoint': ['/bin/bash',\n",
       "      '/opt/ml/processing/input/entrypoint/runproc.sh']},\n",
       "    'RoleArn': 'arn:aws:iam::349674815289:role/service-role/AmazonSageMaker-ExecutionRole-20231113T170180',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-349674815289/Llama2FMOpsPipeline-6e737/code/5098549aa8d474da89fa5af4cc87ec60/sourcedir.tar.gz',\n",
       "       'LocalPath': '/opt/ml/processing/input/code/',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'entrypoint',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-349674815289/Llama2FMOpsPipeline-6e737/code/bc2536a25d34e1ecae5238f42f4207c2/runproc.sh',\n",
       "       'LocalPath': '/opt/ml/processing/input/entrypoint',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Get': 'Parameters.TrainingDatasetS3LocParam'},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'eval_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Get': 'Parameters.EvalDatasetS3LocParam'},\n",
       "        'LocalPath': '/opt/ml/processing/eval',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': 'T12H'}},\n",
       "  {'Name': 'Llama2Train',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': {'Get': 'Parameters.ModelOutputS3LocParam'}},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 300,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingJobInstanceType'}},\n",
       "    'RoleArn': 'arn:aws:iam::349674815289:role/service-role/AmazonSageMaker-ExecutionRole-20231113T170180',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.LLMDataPreprocess.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'training'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.LLMDataPreprocess.ProcessingOutputConfig.Outputs['eval_data'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'validation'}],\n",
       "    'HyperParameters': {'model_id': {'Get': 'Parameters.ModelId'},\n",
       "     'epochs': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.Epochs'}]}},\n",
       "     'per_device_train_batch_size': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.PerDeviceTrainBatchSize'}]}},\n",
       "     'per_device_eval_batch_size': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.PerDeviceEvalBatchSize'}]}},\n",
       "     'learning_rate': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.LearningRate'}]}},\n",
       "     'optimizer': {'Get': 'Parameters.Optimizer'},\n",
       "     'logging_steps': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.LoggingSteps'}]}},\n",
       "     'lora_r': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.LoraR'}]}},\n",
       "     'lora_alpha': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.LoraAlpha'}]}},\n",
       "     'lora_dropout': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.LoraDropout'}]}},\n",
       "     'use_4bit': {'Std:Join': {'On': '',\n",
       "       'Values': [{'Get': 'Parameters.Use4Bit'}]}},\n",
       "     'bnb_4bit_compute_dtype': {'Get': 'Parameters.BnB4BitComputeType'},\n",
       "     'bnb_4bit_quant_type': {'Get': 'Parameters.BnB4BitQuantType'},\n",
       "     'base_model_group_name': {'Get': 'Parameters.BaseModelRegistryGroupName'},\n",
       "     'region': {'Get': 'Parameters.RegionNameParam'},\n",
       "     'model_eval_s3_loc': {'Get': 'Parameters.ModelEvalS3LocParam'},\n",
       "     'run_experiment': '\"False\"',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-west-2-349674815289/Llama2FMOpsPipeline-6e737/code/13c328bf74b0cb858c10c5bcfd093c7e/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-west-2\"'},\n",
       "    'Environment': {'HUGGINGFACE_HUB_CACHE': '/tmp/.cache'},\n",
       "    'DebugHookConfig': {'S3OutputPath': {'Get': 'Parameters.ModelOutputS3LocParam'},\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': {'Get': 'Parameters.ModelOutputS3LocParam'},\n",
       "     'DisableProfiler': False}},\n",
       "   'DependsOn': ['LLMDataPreprocess']},\n",
       "  {'Name': 'CreateModel-CreateModel',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::349674815289:role/service-role/AmazonSageMaker-ExecutionRole-20231113T170180',\n",
       "    'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118',\n",
       "     'Environment': {'SAGEMAKER_PROGRAM': '',\n",
       "      'SAGEMAKER_SUBMIT_DIRECTORY': '',\n",
       "      'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "      'SAGEMAKER_REGION': 'us-west-2',\n",
       "      'SAGEMAKER_MODEL_SERVER_WORKERS': '1'},\n",
       "     'ModelDataUrl': {'Get': 'Steps.Llama2Train.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "  {'Name': 'RegisterModel-RegisterModel',\n",
       "   'Type': 'RegisterModel',\n",
       "   'Arguments': {'ModelPackageGroupName': 'NousResearch-Llama-2-7b-chat-hf-6e737',\n",
       "    'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "       'S3Uri': 's3://sagemaker-us-west-2-349674815289/data/workshop-6e737/modeleval/evaluation.json'}},\n",
       "     'Bias': {},\n",
       "     'Explainability': {}},\n",
       "    'CustomerMetadataProperties': {'training-image-uri': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04'},\n",
       "    'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118',\n",
       "       'Environment': {'SAGEMAKER_PROGRAM': '',\n",
       "        'SAGEMAKER_SUBMIT_DIRECTORY': '',\n",
       "        'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "        'SAGEMAKER_REGION': 'us-west-2',\n",
       "        'SAGEMAKER_MODEL_SERVER_WORKERS': '1'},\n",
       "       'ModelDataUrl': {'Get': 'Steps.Llama2Train.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'Framework': 'PYTORCH',\n",
       "       'FrameworkVersion': '2.0.0'}],\n",
       "     'SupportedContentTypes': ['application/json'],\n",
       "     'SupportedResponseMIMETypes': ['application/json'],\n",
       "     'SupportedRealtimeInferenceInstanceTypes': ['ml.p2.16xlarge',\n",
       "      'ml.p3.16xlarge',\n",
       "      'ml.g4dn.4xlarge',\n",
       "      'ml.g4dn.8xlarge',\n",
       "      'ml.g4dn.12xlarge',\n",
       "      'ml.g4dn.16xlarge',\n",
       "      'ml.g5.2xlarge',\n",
       "      'ml.g5.12xlarge']},\n",
       "    'ModelApprovalStatus': 'PendingManualApproval',\n",
       "    'SkipModelValidation': 'None'}}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204c77f-c1db-4e75-9a23-41043ea01287",
   "metadata": {},
   "source": [
    "# Submit the pipeline to SageMaker and start execution\n",
    "Submit the pipeline definition to the Pipeline service. The Pipeline service uses the role that is passed in to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52b79139-0ead-44f9-91de-60c818e33524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded src/preprocess to s3://sagemaker-us-west-2-349674815289/Llama2FMOpsPipeline-6e737/code/5098549aa8d474da89fa5af4cc87ec60/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-west-2-349674815289/Llama2FMOpsPipeline-6e737/code/bc2536a25d34e1ecae5238f42f4207c2/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:349674815289:pipeline/Llama2FMOpsPipeline-6e737',\n",
       " 'ResponseMetadata': {'RequestId': 'aaf1e96b-7301-4201-be7e-aad6fad77b40',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'aaf1e96b-7301-4201-be7e-aad6fad77b40',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '93',\n",
       "   'date': 'Fri, 17 Nov 2023 18:48:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737086-2bc9-44d6-9a6f-b5df25db55f5",
   "metadata": {},
   "source": [
    "Start the pipeline and accept all the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc63dc3-f6d5-444d-bc87-8275c1391eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01489596-a6df-4b1e-8484-0ca52e8753f8",
   "metadata": {},
   "source": [
    "Wait for the pipeline to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f11a94d7-bc86-40f1-b287-535dbf5ba784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d6746-8108-42ec-a70b-2dc0b831eb43",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Project\n",
    "SageMaker Projects help organizations set up and standardize developer environments for data scientists and CI/CD systems for MLOps engineers. \n",
    "\n",
    "Projects also help organizations set up dependency management, code repository management, build reproducibility, and artifact sharing.\n",
    "You can provision SageMaker Projects from the AWS Service Catalog using custom or SageMaker-provided templates. \n",
    "\n",
    "For information about the AWS Service Catalog, see What Is [AWS Service Catalog](https://docs.aws.amazon.com/servicecatalog/latest/dg/what-is-service-catalog.html). \n",
    "\n",
    "With SageMaker Projects, MLOps engineers and organization admins can define their own templates or use SageMaker-provided templates. \n",
    "\n",
    "The SageMaker-provided templates bootstrap the ML workflow with source version control, automated ML pipelines, and a set of code to quickly start iterating over ML use cases.\n",
    "\n",
    "Here's an architecture diagram that shows the components of a SageMaker Project:\n",
    "\n",
    "![sagemaker project](images/sagemaker-project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367d111-7250-4732-8054-17ddec2e1d55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create a SageMaker Project\n",
    "In this lab, we'll use a SageMaker project to help us create the infrastructure for creating an LLM deployment pipeline. Specifically, we'll leverage SageMaker project to create the following main components:\n",
    "\n",
    "1. A Git repository using AWS CodeCommit for code and configuration for the model deployment. \n",
    "2. A CICD pipeline using AWS CodePipeline orchestrates the deployment process.\n",
    "3. A build project using AWS CodeBuild to create a CloudFormation template based on the given configuration.\n",
    "4. Event Bridge Rule with AWS Event Bridge that triggers an LLM deployment based on the approval status update made in the SageMaker Model Registry\n",
    "\n",
    "    \n",
    "In the following section, we'll walk through the steps for creating a SageMaker Project. Before that, we need to capture the ```model registry package group name``` which was created in the SageMaker Pipeline step. \n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Important:</strong> Please make sure to use the correct SageMaker model package group name when creating a SageMaker project. You can find the the model registry package group name in a variable called <strong>model_package_group_name</strong> defined previously in the lab. The following cell prints the value so you could reference the value accordingly. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc4a534f-6442-490b-a170-7ada208f4b97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model package group name to be used for the SageMaker Project: \u001b[6;30;42mNousResearch-Llama-2-7b-chat-hf-6e737\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model package group name to be used for the SageMaker Project: \\x1b[6;30;42m{model_package_group_name}\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0a52c-bb04-424c-a079-97d93da0cc2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting Up SageMaker Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f989d9e-1450-421b-9847-4f2823934965",
   "metadata": {},
   "source": [
    "In this lab, we'll leverage SageMaker Project to create the a CICD pipeline that integreates with the LLM training pipeline. We are going to go through the following steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f718d-2c61-4783-8a86-2d98627f97a3",
   "metadata": {},
   "source": [
    "#### 1. Retrieve the `model_package_group_name` that was registered in SageMaker Model Registry. This value should be stored in `model_package_group_name` variable in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c8fc5-b7b4-40a7-ab17-193877ffed42",
   "metadata": {},
   "source": [
    "#### 2. Navigate to SageMaker project from Studio as shown in the following diagram:\n",
    "\n",
    "<img src=\"images/sagemaker-project-studio-ui.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28063d09-7ad0-4c91-b148-35621d1866f9",
   "metadata": {},
   "source": [
    "#### 3. Create a new SageMaker Project by clicking on *Create Project* button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160aaf9-9580-43ba-a5d4-c0170f3f6e6b",
   "metadata": {},
   "source": [
    "#### 4. Select `Model deployment` in the list, as shown in the following diagram:\n",
    "\n",
    "<img src=\"images/sm-project-create.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bb23e-4a6b-4c32-9818-6a9aabf351c6",
   "metadata": {},
   "source": [
    "#### 5. Provide a *unique project name* and the model package group name value shown in the previous cell:\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Important:</strong> Please make sure to use the correct SageMaker model package group name when in this step SageMaker project. You can find the the model registry package group name in the variable named <strong>model_package_group_name</strong> defined previously in this notebook. As shown in the screenshot below, my model registry package group name is <strong>NousResearch-Llama-2-7b-chat-hf-67140</strong>. Yours will be different than mine, but the naming convention should be similar.\n",
    "</div>\n",
    "\n",
    "<img src=\"images/sm-project-template.png\" width=\"500\">\n",
    "\n",
    "*Note:* Make sure your project name is unique to avoid any conflict with other projects already exists in the AWS environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743c3a5-732d-4253-b16e-554f1e68da05",
   "metadata": {},
   "source": [
    "### 6. Click on the newly created project and clone the repository into your SageMaker Studio environment\n",
    "\n",
    "<img src=\"images/sm-project-clone-repository.png\" width=\"500\">\n",
    "<img src=\"images/sm-project-clone-repo.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e03fe-b46b-4afb-92d0-f7c0413f7ee4",
   "metadata": {},
   "source": [
    "#### 7. After cloning the repository, you should see the gitcommit repository folder created:\n",
    "\n",
    "<img src=\"images/sm-project-local-clone.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b58c9-efa4-4b21-82d2-82d4ef6bd252",
   "metadata": {},
   "source": [
    "#### 8. Since the SageMaker project repository was created via a template, we need to make a minor change to the configuration so the CodePipeline can adapt the LLM which we've been working with. In the project git repository folder that we cloned in the earlier step, open the file named `staging-config.json` using an edit as shown in the following:\n",
    "<img src=\"images/sm-project-staging-update.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291650da-ae9d-454a-9ae9-3fa1374f36aa",
   "metadata": {},
   "source": [
    "#### 9. Update the following variables:\n",
    "\n",
    "* EndpointInstanceType: \"ml.g5.2xlarge\"\n",
    "* EnableDataCapture: \"true\"\n",
    "\n",
    "<img src=\"images/sm-project-staging-change.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6896af6-3734-416e-8ff0-6765ba52139e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10. Save the changes (by Ctrl-S on Windows, or Command-S, or File->Save JSON File)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d7861-d71e-47a9-8bdc-485593f6ca88",
   "metadata": {},
   "source": [
    "#### 11. Navigate to Git console from the left panel and stage the changes by clicking the '+' sign as shown in the following diagram:\n",
    "\n",
    "![stage-changes](images/sm-project-stage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197a9b0-c0de-4445-bc32-650ebb1d991b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 12. Commit the changes by adding a brief description of the change, and hit the `Commit` button at the end.\n",
    "\n",
    "<img src=\"images/sm-project-commit.png\" width=\"300\">\n",
    "<img src=\"images/sm-project-commit-email.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3568c85-34bc-4c83-af55-868a4ba3ed40",
   "metadata": {},
   "source": [
    "#### 13. Push the changes into AWS Codecommit as shown in the diagram below. A successful push message will appear in the lower right hand corner of the screen.\n",
    "\n",
    "<img src=\"images/sm-project-git-push.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117b11f-447f-4cb5-b619-ce7f1ab9a7c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Model Registry Status Update \n",
    "With the success of AWS CodePipeline update, next, we will verify the deployment process by approving the model that we have registered in the SageMaker Model Registry via SageMaker Pipeline execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39db22-ae28-4dde-9a08-dbd45abe619a",
   "metadata": {},
   "source": [
    "#### 1. Locate the Model Package Group Name that you've created in the SageMaker Pipeline Run:\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "<strong>Important:</strong> You can find the the model registry package group name in the variable named <strong>model_package_group_name</strong> defined previously in this notebook. As shown in the screenshot below, my model registry package group name is <strong>NousResearch-Llama-2-7b-chat-hf-67140</strong>. Yours will be different than mine, but the naming convention should be similar.\n",
    "</div>\n",
    "\n",
    "Your model package group name is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf3bf7d5-4126-4419-bd55-cafec01e2001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model package group name is: NousResearch-Llama-2-7b-chat-hf-6e737\n"
     ]
    }
   ],
   "source": [
    "print(f\"Your model package group name is: {model_package_group_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1764a07-7764-4406-a64b-63ad1a9af9e1",
   "metadata": {},
   "source": [
    "![sm-model-registry-ui](images/model-registry-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6525296-0052-4442-8974-4afa1b605b10",
   "metadata": {},
   "source": [
    "#### 2. By default, the status of the registered model is in \"Pending Approval\". When the model is approved, it'll trigger a model deployment job in CodePipeline. To update the status, edit the model version as shown below:\n",
    "\n",
    "<img src=\"images/model-registry-approval-edit.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72c294-eb0d-4814-9d0c-66eb9b55e84c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Change the status to `Approve`, and save the changes.\n",
    "\n",
    "<img src=\"images/model-registry-approve-save.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3929882-b9b2-4ab5-aa9a-928bc251e0ed",
   "metadata": {},
   "source": [
    "#### 4. A new CodePipeline job should be triggered based on the approval status update from the previous step. To verify the pipeline execution, navigate to AWS CodePipeline console\n",
    "\n",
    "![code-pipeline console](images/codepipeline-aws-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ecec2-63c6-4446-83d0-faacd1468eb9",
   "metadata": {},
   "source": [
    "#### 5. Click on the pipeline that was created for your project:\n",
    "\n",
    "![code-pipeline-status-check](images/codepipeline-status-check.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d278c6-db42-40e3-b725-5eb1cdc35c3e",
   "metadata": {},
   "source": [
    "#### 6. The LLM deployment should be triggered. It takes about 10 minutes to deploy the model in SageMaker Hosting. You can track the status in the pipeline\n",
    "directly in the CodePipeline UI as shown in the following:\n",
    "\n",
    "![code-pipeline-deploy](images/codepipeline-staging-deploy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f6025-d5a1-432a-940f-6031b2f5fbe4",
   "metadata": {},
   "source": [
    "#### 7. Congratulations! You've successfully completed the model deployment pipeline for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a09f86-03b3-4690-af94-6e89659381cf",
   "metadata": {},
   "source": [
    "#### 8. Find out the endpoint name\n",
    "After the endpoint is deployed, you can find the endpoint name from the SageMaker Studio:\n",
    "\n",
    "Go to ![home](images/home.png) -> Deployment -> Endpoints\n",
    "\n",
    "![endpoint](images/sm-studio-deployment-endpoint.png)\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "<strong>Note:</strong> Capture the endpoint name as you see in the endpoint, we will use this endpoint in the subsequent labs to build a generative AI application using the LLM endpoint. In the example above, the endpoint name for my deployment is <strong>hf-llama2-b987c-pipeline-staging</strong>. Your endpoint name will be different. Copy your endpoint name, we will store the variable into the session so that we could reference it in the later labs. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "830ce837-63b3-4ab5-b95d-7a2314ffa311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name = \"hf-llama2-b987c-pipeline-staging\".strip() # put your endpoint name here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb6cfeb9-a58b-40bd-b325-e96dbbb23586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'llm_endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store llm_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46230d8-1286-40d0-a3cf-dda1da53cbdf",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "In this lab, we saw how we can use SageMaker Pipelines to automate the LLM model training/finetuning pipeline, and integrates with a CICD pipeline created using SageMaker Project orchestrate model deployment process at scale using AWS CodePipeline. \n",
    "\n",
    "We'll keep the endpoint running for the remaining of the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e82245-5552-4282-a0c2-27a981784134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
