{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bc5b75-77e9-436b-857b-f28a2e118e29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automate LLM training and deployment pipeline using SageMaker Pipelines\n",
    "\n",
    "Amazon SageMaker Pipelines offers machine learning (ML) application developers and operations engineers the ability to orchestrate SageMaker jobs and author reproducible ML pipelines. It also enables them to deploy custom-built models for inference in real-time with low latency, run offline inferences with Batch Transform, and track lineage of artifacts. They can institute sound operational practices in deploying and monitoring production workflows, deploying model artifacts, and tracking artifact lineage through a simple interface, adhering to safety and best practice paradigms for ML application development.\n",
    "\n",
    "In the previous lab, we saw how we could use SageMaker to simplify the data processing, finetuning an LLM and deploy the finetuned model and run inferences. In this lab, we'll use SageMaker Pipelines to help us automate the entire LLM training and deployment process using serverless and event driven architecture. \n",
    "\n",
    "Here's a high level architecture diagram for the LLMOps workflow:\n",
    "\n",
    "<img src=\"images/mlops-llm.drawio.png\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713c6ec-5435-4090-bb2b-d797b4563db4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook shows how to:\n",
    "\n",
    "* Define a set of Pipeline parameters that can be used to parametrize a SageMaker Pipeline.\n",
    "* Define a Processing step that performs feature engineering of a Huggingface dataset and split the dataset into train and evaluation data sets.\n",
    "* Define a Training step that finetunes a llama2-7b model on the preprocessed data set.\n",
    "* Define a Create Model step that creates a model from the model artifacts used in training.\n",
    "* Define a Register Model step that creates a model package from the estimator and model artifacts used to finetune the model.\n",
    "* Define and create a Pipeline definition in a DAG, with the defined parameters and steps.\n",
    "* Start a Pipeline execution and wait for execution to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ff05a-95ba-4309-8fbd-68bb99811fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install 'sagemaker' --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e82187-e374-4db6-9349-d5a514fd62c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "region = pipeline_session.boto_region_name\n",
    "default_bucket = pipeline_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cc819-68e1-4908-8f79-4c36300c8c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger, ParameterBoolean\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "processing_instance_count = 1\n",
    "training_instance_count = 1\n",
    "transform_instance_count = 1\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "rand_id = uuid.uuid4().hex[:5] # this is the random-id assigned for each run. \n",
    "training_dataset_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/train\"\n",
    "validation_dataset_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/eval\"\n",
    "model_output_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/model\"\n",
    "model_eval_s3_loc = f\"s3://{default_bucket}/data/workshop-{rand_id}/modeleval\"\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "hf_dataset_name = \"hotpot_qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28db46-c557-4335-9496-b6466eed4bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"T12H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b6822-1b5e-450c-9899-aa4a3c7fec76",
   "metadata": {},
   "source": [
    "# Define Parameters to parametize SageMaker Pipeline Executions\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "* ParameterInteger - represents an int Python type\n",
    "* ParameterFloat - represents a float Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow include:\n",
    "\n",
    "* processing_output_s3_location_param - processing job output S3 location. \n",
    "* model_id_param - huggingface model ID to indentify the base model\n",
    "* epochs_param - number of epochs to finetune the model\n",
    "* per_device_train_batch_size_param - training dataset batch size \n",
    "* per_device_eval_batch_size_param - evaluation dataset batch size  \n",
    "* learning_rate_param - QLoRA finetuning learning rate\n",
    "* optimizer_param - the model optimizer\n",
    "* logging_steps_param - number of steps for logging\n",
    "* lora_r_param - LoRA attention dimension\n",
    "* lora_alpha_param - Alpha parameter for LoRA scaling\n",
    "* lora_dropout_param - LoRA dropout rate\n",
    "* use_4bit_param - load base model in 4bit\n",
    "* bnb_4bit_compute_dtype_param - default compute type for quantization\n",
    "* bnb_4bit_quant_type_param - default quantization type\n",
    "* training_job_instance_type_param - training job instance type\n",
    "* model_output_s3_loc_param - model artifact output S3 location\n",
    "* training_dataset_s3_loc_param - S3 location for training dataset\n",
    "* eval_dataset_s3_loc_param - S3 location for evaluation dataset\n",
    "* training_dataset_split_param - training dataset split configuration\n",
    "* eval_dataset_split_param - evaluation dataset split configuration\n",
    "* base_model_group_name_param - base model package group name\n",
    "* region_param - AWS region name\n",
    "* model_eval_s3_loc_param - S3 location for model evalution metrics JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd50e7b-9c5a-474b-9d40-80a430209af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861754e5-b4a3-4ccb-b468-0e6e5f28b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_output_s3_location_param = ParameterString(name=\"ProcessingOutputS3Location\", default_value=training_dataset_s3_loc)\n",
    "model_id_param = ParameterString(name=\"ModelId\", default_value=model_id)\n",
    "epochs_param = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "per_device_train_batch_size_param = ParameterInteger(name=\"PerDeviceTrainBatchSize\", default_value=8)\n",
    "per_device_eval_batch_size_param = ParameterInteger(name=\"PerDeviceEvalBatchSize\", default_value=8)\n",
    "learning_rate_param = ParameterFloat(name=\"LearningRate\", default_value=2e-4)\n",
    "optimizer_param = ParameterString(name=\"Optimizer\", default_value=\"paged_adamw_32bit\")\n",
    "logging_steps_param = ParameterInteger(name=\"LoggingSteps\", default_value=25)\n",
    "lora_r_param = ParameterInteger(name=\"LoraR\", default_value=64)\n",
    "lora_alpha_param = ParameterInteger(name=\"LoraAlpha\", default_value=16)\n",
    "lora_dropout_param = ParameterFloat(name=\"LoraDropout\", default_value=0.1)\n",
    "use_4bit_param = ParameterBoolean(name=\"Use4Bit\", default_value=True)\n",
    "bnb_4bit_compute_dtype_param = ParameterString(name=\"BnB4BitComputeType\", default_value=\"float16\")\n",
    "bnb_4bit_quant_type_param = ParameterString(name=\"BnB4BitQuantType\", default_value=\"nf4\")\n",
    "training_job_instance_type_param = ParameterString(name=\"TrainingJobInstanceType\", default_value=\"ml.g5.2xlarge\")\n",
    "processing_job_instance_type_param = ParameterString(name=\"ProcessingJobInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_output_s3_loc_param = ParameterString(name=\"ModelOutputS3LocParam\", default_value=model_output_s3_loc)\n",
    "training_dataset_s3_loc_param = ParameterString(name=\"TrainingDatasetS3LocParam\", default_value=training_dataset_s3_loc)\n",
    "eval_dataset_s3_loc_param = ParameterString(name=\"EvalDatasetS3LocParam\", default_value=validation_dataset_s3_loc)\n",
    "training_dataset_split_param = ParameterString(name=\"TrainingDatasetSplitParam\", default_value=\"1:50\")\n",
    "eval_dataset_split_param = ParameterString(name=\"EvalDatasetSplitParam\", default_value=\"51:100\")\n",
    "base_model_group_name_param = ParameterString(name=\"BaseModelRegistryGroupName\", default_value=\"None\")\n",
    "region_param = ParameterString(name=\"RegionNameParam\", default_value=\"us-east-1\")\n",
    "model_eval_s3_loc_param = ParameterString(name=\"ModelEvalS3LocParam\", default_value=model_eval_s3_loc)\n",
    "hf_dataset_name_param = ParameterString(name=\"HFDataSetNameParam\", default_value=hf_dataset_name)\n",
    "base_model_package_group_name_param = ParameterString(name=\"BaseModelPkgGoupName\", default_value=base_model_pkg_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9941bc9-f69e-491d-9540-d67b68c04ebe",
   "metadata": {},
   "source": [
    "# Define a Processing Step\n",
    "A processing step is used for triggering a processing job for data processing. \n",
    "In this example, we are going to use a processing job to perform feature engineering on a public dataset available on Huggingface Hub. The output from the processing step will be stored in the specified S3 location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d7538-faa6-4c97-8c7a-765214d382f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "torch_processor = PyTorchProcessor(\n",
    "    framework_version='2.0',\n",
    "    role=role,\n",
    "    instance_type=processing_job_instance_type_param,\n",
    "    instance_count=1,\n",
    "    base_job_name=f'frameworkprocessor-PT-{rand_id}',\n",
    "    py_version=\"py310\",\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86993ffe-66df-4f4e-bc6b-c5f0bf2bf35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor_args = torch_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    source_dir=\"src/preprocess\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\",\n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=training_dataset_s3_loc_param),\n",
    "        ProcessingOutput(output_name=\"eval_data\",\n",
    "                         source=\"/opt/ml/processing/eval\",\n",
    "                         destination=eval_dataset_s3_loc_param),\n",
    "\n",
    "    ],\n",
    "    arguments=[\"--train-data-split\", training_dataset_split_param,\n",
    "               \"--eval-data-split\", eval_dataset_split_param,\n",
    "               \"--hf-dataset-name\", hf_dataset_name_param]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc503b7-9475-4fa9-8acb-114971102bec",
   "metadata": {},
   "source": [
    "Define a procesing step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5677e-20e0-4ac9-baf0-c660805f9dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_process = ProcessingStep(name=\"LLMDataPreprocess\",\n",
    "                              step_args=processor_args,\n",
    "                              cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf07e59-a2ed-40fa-8dbf-69c94e5eec2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Step\n",
    "In this section, use define a training step to finetune a Llama2-7b model on the given dataset. \n",
    "Configure an Estimator for the HuggingFace and the input dataset. \n",
    "A typical training script loads data from the input channels, configures training with \n",
    "hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later.\n",
    "\n",
    "The model path where the models from training are saved is also specified.\n",
    "\n",
    "**Note:** the instance_type parameter may be used in multiple places in the pipeline. In this case, the instance_type is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bf0dd-a5be-4b2d-8cb7-abf4de3b70f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}-{rand_id}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id_param,                                # pre-trained model\n",
    "  'epochs': epochs_param,                                     # number of training epochs\n",
    "  'per_device_train_batch_size': per_device_train_batch_size_param,\n",
    "  'per_device_eval_batch_size' : per_device_eval_batch_size_param,\n",
    "  'learning_rate' : learning_rate_param,\n",
    "  'optimizer' : optimizer_param,  \n",
    "  'logging_steps' : logging_steps_param,\n",
    "  'lora_r': lora_r_param,\n",
    "  'lora_alpha' : lora_alpha_param,\n",
    "  'lora_dropout' : lora_dropout_param, \n",
    "  'use_4bit' : use_4bit_param,\n",
    "  'bnb_4bit_compute_dtype' : bnb_4bit_compute_dtype_param,\n",
    "  'bnb_4bit_quant_type' : bnb_4bit_quant_type_param,\n",
    "  'base_model_group_name' : base_model_group_name_param,\n",
    "  'region' : region_param,\n",
    "  'model_eval_s3_loc' : model_eval_s3_loc_param,\n",
    "  'run_experiment' : \"False\"\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',         # train script\n",
    "    source_dir='src/train',         # directory which includes all the files needed for training\n",
    "    instance_type=training_job_instance_type_param, # instances type used for the training job\n",
    "    instance_count=1,               # the number of instances used for training\n",
    "    base_job_name=job_name,         # the name of the training job\n",
    "    role=role,      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,    # the size of the EBS volume in GB\n",
    "    transformers_version='4.28.1',    # the transformers version used in the training job\n",
    "    pytorch_version='2.0.0',          # the pytorch_version version used in the training job\n",
    "    py_version='py310',             # the python version used in the training job\n",
    "    hyperparameters= hyperparameters,\n",
    "    environment={ \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    sagemaker_session=pipeline_session,         # specifies a sagemaker session object\n",
    "    output_path=model_output_s3_loc_param # s3 location for model artifact\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3acdb1-7772-4372-ace1-2002b07da0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {'training': step_process.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "        'validation': step_process.properties.ProcessingOutputConfig.Outputs[\"eval_data\"].S3Output.S3Uri\n",
    "       }\n",
    "\n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}-{rand_id}\"\n",
    "# starting the train job with our uploaded datasets as input\n",
    "train_args = huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897109aa-bbab-4dac-bfa9-b1505233ea4a",
   "metadata": {},
   "source": [
    "Define the Training step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0850c8-c8c2-4701-97eb-6110b8b28518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Llama2Train\",\n",
    "    step_args=train_args,\n",
    ")\n",
    "step_train.add_depends_on([step_process])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c145be-b6a8-4e8d-8b56-5a1b803f1941",
   "metadata": {},
   "source": [
    "# Register Model\n",
    "SageMaker Model Registry supports the following features and functionality:\n",
    "\n",
    "* Catalog models for production.\n",
    "* Manage model versions. \n",
    "* Associate metadata, such as training metrics, with a model.\n",
    "* Manage the approval status of a model.\n",
    "* Deploy models to production.\n",
    "* Automate model deployment with CI/CD.\n",
    "\n",
    "In this workshop, we are going to register the finetuned LLama2 model as a model package using SageMaker Model Registry. \n",
    "\n",
    "A model package is an abstraction of reusable model artifacts that packages all ingredients required for inference. \n",
    "Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cef850-7542-411f-8984-5b8b9d9db790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import json\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    image_uri=llm_image,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    name=f\"HuggingFaceModel-Llama2-7b-{rand_id}\",\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "create_step_args = huggingface_model.create(instance_type=inference_instance_type)\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    step_args=create_step_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80497d2b-5550-48be-b9fb-64d02a0ca3d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Metrics\n",
    "To capture the model training and evalution metrics from a SageMaker Training job, we use a `ModelMetrics` class. We captured the model evaluation metrics in a `evaluation.json`, stored in the specified S3 location. With that information, we create a `ModelMetrics` object to include the metrics. The object is used to register the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe56123-405d-4c8e-a345-4b5afa42da50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "import os \n",
    "\n",
    "model_package_group_name = f\"NousResearch-Llama-2-7b-chat-hf-{rand_id}\"\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=os.path.join(model_eval_s3_loc, \"evaluation.json\"),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\", \n",
    "        \"ml.g5.2xlarge\",\n",
    "        \"ml.g5.12xlarge\",\n",
    "    ],\n",
    "    model_package_group_name=f\"NousResearch-Llama-2-7b-chat-hf-{rand_id}\",\n",
    "    customer_metadata_properties = {\"training-image-uri\": huggingface_estimator.training_image_uri()},  #Store the training image url\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "step_register = ModelStep(name=\"RegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1269da1-c952-406f-b4eb-60ec2e60c3fe",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters and Steps \n",
    "In this section, we combine all the steps into a Pipeline so it can be executed.\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the data dependency DAG as steps for the execution to complete.\n",
    "* Steps must be unique to across the pipeline step list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0c07f-04ad-4960-887e-0edcee2216f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_experiment_config import PipelineExperimentConfig\n",
    "\n",
    "pipeline_name = f\"Llama2FMOpsPipeline-{rand_id}\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_output_s3_location_param,\n",
    "        model_id_param,\n",
    "        epochs_param,\n",
    "        per_device_train_batch_size_param,\n",
    "        per_device_eval_batch_size_param,\n",
    "        learning_rate_param,\n",
    "        optimizer_param,\n",
    "        logging_steps_param,\n",
    "        lora_r_param,\n",
    "        lora_alpha_param,\n",
    "        lora_dropout_param,\n",
    "        use_4bit_param,\n",
    "        bnb_4bit_compute_dtype_param,\n",
    "        bnb_4bit_quant_type_param,\n",
    "        training_job_instance_type_param,\n",
    "        model_output_s3_loc_param,\n",
    "        training_dataset_s3_loc_param,\n",
    "        eval_dataset_s3_loc_param,\n",
    "        training_dataset_split_param,\n",
    "        eval_dataset_split_param,\n",
    "        base_model_group_name_param,\n",
    "        region_param,\n",
    "        model_eval_s3_loc_param,\n",
    "        hf_dataset_name_param,\n",
    "        processing_job_instance_type_param,\n",
    "        base_model_package_group_name_param\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model,step_register],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa337cc9-4f2f-42d4-a54f-44426cdb5741",
   "metadata": {},
   "source": [
    "# Examining the pipeline definition\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b1c2e-42b4-434e-96f5-359f9338365e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204c77f-c1db-4e75-9a23-41043ea01287",
   "metadata": {},
   "source": [
    "# Submit the pipeline to SageMaker and start execution\n",
    "Submit the pipeline definition to the Pipeline service. The Pipeline service uses the role that is passed in to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b79139-0ead-44f9-91de-60c818e33524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737086-2bc9-44d6-9a6f-b5df25db55f5",
   "metadata": {},
   "source": [
    "Start the pipeline and accept all the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc63dc3-f6d5-444d-bc87-8275c1391eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01489596-a6df-4b1e-8484-0ca52e8753f8",
   "metadata": {},
   "source": [
    "Wait for the pipeline to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a94d7-bc86-40f1-b287-535dbf5ba784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d6746-8108-42ec-a70b-2dc0b831eb43",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Project\n",
    "SageMaker Projects help organizations set up and standardize developer environments for data scientists and CI/CD systems for MLOps engineers. \n",
    "\n",
    "Projects also help organizations set up dependency management, code repository management, build reproducibility, and artifact sharing.\n",
    "You can provision SageMaker Projects from the AWS Service Catalog using custom or SageMaker-provided templates. \n",
    "\n",
    "For information about the AWS Service Catalog, see What Is [AWS Service Catalog](https://docs.aws.amazon.com/servicecatalog/latest/dg/what-is-service-catalog.html). \n",
    "\n",
    "With SageMaker Projects, MLOps engineers and organization admins can define their own templates or use SageMaker-provided templates. \n",
    "\n",
    "The SageMaker-provided templates bootstrap the ML workflow with source version control, automated ML pipelines, and a set of code to quickly start iterating over ML use cases.\n",
    "\n",
    "Here's an architecture diagram that shows the components of a SageMaker Project:\n",
    "\n",
    "![sagemaker project](images/sagemaker-project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367d111-7250-4732-8054-17ddec2e1d55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create SageMaker Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a534f-6442-490b-a170-7ada208f4b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"model package group name: {model_package_group_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f989d9e-1450-421b-9847-4f2823934965",
   "metadata": {},
   "source": [
    "In this lab, we'll leverage SageMaker Project to create the a CICD pipeline that integreates with the LLM training pipeline. We are going to go through the following steps:\n",
    "\n",
    "1. Retrieve the `model_package_group_name` that was registered in SageMaker Model Registry. This value should be stored in `model_package_group_name` variable in this notebook. \n",
    "2. Navigate to SageMaker project from Studio as shown in the following diagram:\n",
    "\n",
    "<img src=\"images/sagemaker-project-studio-ui.png\" width=\"250\">\n",
    "3. Create a new SageMaker Project by clicking on *Create Project* button\n",
    "4. Select `Model deployment` in the list, as shown in the following diagram:\n",
    "\n",
    "<img src=\"images/sm-project-create.png\" width=\"500\">\n",
    "\n",
    "5. Provide a *unique project name* and the model package group name value shown in the previous cell\n",
    "\n",
    "<img src=\"images/sm-project-template.png\" width=\"500\">\n",
    "\n",
    "*Note:* Make sure your project name is unique to avoid any conflict with other projects already exists in the AWS environment.\n",
    "\n",
    "6. Click on the newly created project and clone the repository into your SageMaker Studio environment\n",
    "\n",
    "<img src=\"images/sm-project-clone-repository.png\" width=\"500\">\n",
    "<img src=\"images/sm-project-clone-repo.png\" width=\"500\">\n",
    "\n",
    "7. After cloning the repository, you should see the gitcommit repository folder created:\n",
    "\n",
    "<img src=\"images/sm-project-local-clone.png\" width=\"1200\">\n",
    "\n",
    "8. Since the SageMaker projet repository was created via a template, we need to make a minor change to the project to adapt to the deployment pipeline that we are working with. In the project repository folder, open the file named `staging-config.json` using an edit as shown in the following:\n",
    "<img src=\"images/sm-project-staging-update.png\" width=\"500\">\n",
    "\n",
    "9. Update the following variables:\n",
    "* EndpointInstanceType: \"ml.g5.2xlarge\"\n",
    "* EnableDataCapture: \"true\"\n",
    "\n",
    "<img src=\"images/sm-project-staging-change.png\" width=\"500\">\n",
    "\n",
    "10. Save the changes (by Ctrl-S on Windows, or Command-S, or File->Save JSON File).\n",
    "\n",
    "11. Navigate to Git console from the left panel and stage the changes as shown in the following diagram:\n",
    "\n",
    "![stage-changes](images/sm-project-stage.png)\n",
    "\n",
    "12. Commit the changes by adding a brief description of the change, and hit the `Commit` button at the end.\n",
    "\n",
    "<img src=\"images/sm-project-commit.png\" width=\"300\">\n",
    "<img src=\"images/sm-project-commit-email.png\" width=\"300\">\n",
    "\n",
    "13. Push the changes into AWS Codecommit as shown in the diagram below. A successful push message will appear in the lower right hand corner of the screen.\n",
    "\n",
    "<img src=\"images/sm-project-git-push.png\" width=\"300\">\n",
    "\n",
    "We have a pipeline successfully created. Now let's test out the deployment process by approving the model that we have registered in the SageMaker Model Registry.\n",
    "\n",
    "1. Locate the Model Package that you've created in the SageMaker Pipeline Run:\n",
    "\n",
    "![sm-model-registry-ui](images/model-registry-ui.png)\n",
    "\n",
    "2. By default, the status of the registered model is in \"Pending Approval\". When the model is approved, it'll trigger a model deployment job in CodePipeline. To update the status, edit the model version as shown below:\n",
    "\n",
    "<img src=\"images/model-registry-approval-edit.png\" width=\"800\">\n",
    "\n",
    "3. Change the status to `Approve`, and save the changes.\n",
    "\n",
    "<img src=\"images/model-registry-approve-save.png\" width=\"800\">\n",
    "\n",
    "4. A new CodePipeline job should be triggered based on the approval status update from the previous step. To verify the pipeline execution, navigate to AWS CodePipeline console\n",
    "\n",
    "![code-pipeline console](images/codepipeline-aws-ui.png)\n",
    "\n",
    "5. Click on the pipeline that was created for your project:\n",
    "\n",
    "![code-pipeline-status-check](images/codepipeline-status-check.png)\n",
    "\n",
    "6. The LLM deployment should be triggered. It takes about 10 minutes to deploy the model in SageMaker Hosting. You can track the status in the pipeline\n",
    "directly in the CodePipeline UI as shown in the following:\n",
    "\n",
    "![code-pipeline-deploy](images/codepipeline-staging-deploy.png)\n",
    "\n",
    "7. Congratulations! You've successfully completed the model deployment pipeline for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46230d8-1286-40d0-a3cf-dda1da53cbdf",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "In this lab, we saw how we can use SageMaker Pipelines to automate the LLM model training/finetuning pipeline, and integrates with a CICD pipeline created using SageMaker Project orchestrate model deployment process at scale using AWS CodePipeline. \n",
    "\n",
    "We'll keep the endpoint running for the remaining of the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e82245-5552-4282-a0c2-27a981784134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
