{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731c3adb",
   "metadata": {},
   "source": [
    "Using this notebook to test different LLM evaluation techniques\n",
    "\n",
    "[ragas framework](https://github.com/explodinggradients/ragas)\n",
    "[llamaindex](https://gpt-index.readthedocs.io/en/v0.6.36/how_to/evaluation/evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fe064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pysbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645263e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handful of configuration\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "import base64\n",
    "from sagemaker import get_execution_role, session\n",
    "import numpy as np\n",
    "\n",
    "import pysbd\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "region= boto3.Session().region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495435b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = <provide endpoint name>\n",
    "default_bucket = <default bucket>\n",
    "current_endpoint_capture_prefix = \"\"\n",
    "s3_key_prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://{default_bucket}/{current_endpoint_capture_prefix} data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cfea3",
   "metadata": {},
   "source": [
    "## Initialize LLM & util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a7132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs={\"max_tokens_to_sample\": 200,\n",
    "                \"temperature\": 0},\n",
    "    client=boto3.client(\"bedrock-runtime\", region_name='us-west-2'),\n",
    ")\n",
    "\n",
    "\n",
    "embeddings= BedrockEmbeddings(\n",
    "    client=boto3.client(\"bedrock-runtime\", region_name='us-west-2'),\n",
    ")\n",
    "\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69ad76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def base64_to_string(base64_string):\n",
    "    base64_bytes = base64_string.encode('ascii')\n",
    "    string_bytes = base64.b64decode(base64_bytes) \n",
    "    return string_bytes.decode('utf-8')\n",
    "\n",
    "def extract_questions(text):\n",
    "    pattern = r\"### Question\\n(.*?)\\n### Context\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match is None:\n",
    "        return \"\"\n",
    "    return match.group(1)\n",
    "\n",
    "def extract_answers(text):\n",
    "    pattern = r\"\\[\\/INST\\](.*)\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match is None:\n",
    "        return \"\"\n",
    "    return match.group(1)   \n",
    "\n",
    "def extract_contexts(text):\n",
    "    pattern = r\"### Context\\n(.*)\\[/INST]\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match is None:\n",
    "        return \"\"\n",
    "    return match.group(1)\n",
    "\n",
    "# Helper function to extract question and answer from dataset\n",
    "def extract_qac(input_data, output_data):\n",
    "    question = extract_questions(json.loads(base64_to_string(input_data))[\"text\"])\n",
    "    print(\"Question: \", question)\n",
    "    context = extract_contexts(json.loads(base64_to_string(input_data))[\"text\"])\n",
    "    print(\"Context: \", context)\n",
    "    generated_text = json.loads(base64_to_string(output_data))[\"outputs\"][0][\"generated_text\"]\n",
    "    answer = extract_answers(generated_text)\n",
    "    print(\"Answer: \", answer)\n",
    "    return question, answer, context\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    \"\"\"\n",
    "    tokenizer text into sentences\n",
    "    \"\"\"\n",
    "    sentences = seg.segment(text)\n",
    "    assert isinstance(sentences, list)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0ce72",
   "metadata": {},
   "source": [
    "## Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d837d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RELEVANCE_TEMPLATE = \"\"\"\\n\\nHuman: Generate question for the given answer.\\n\\nAssistant:Okay, give me an answer, and I will generate a question.\n",
    "\\nHuman:Answer:\\nThe PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India \n",
    "\\nAssistant:Question:\\nWhen is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?\n",
    "\\nHuman:Answer:\\n{answer}\n",
    "\\nAssistant:Question:\\n\n",
    "\"\"\" \n",
    "\n",
    "EVALUATOR = PromptTemplate(template=RELEVANCE_TEMPLATE, input_variables=[\"answer\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36caa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_dir = \"./workspace/data\"\n",
    "questions, answers, contexts = [], [], []\n",
    "\n",
    "for filepath in pathlib.Path(infer_dir).rglob('*.jsonl'):\n",
    "\n",
    "    with open(filepath.absolute(), 'r') as f:\n",
    "        for line in f:\n",
    "            jsonl = json.loads(line)\n",
    "            input_data = jsonl['captureData']['endpointInput']['data']\n",
    "            output_data = jsonl['captureData']['endpointOutput']['data']\n",
    "            \n",
    "            q, a, c = extract_qac(input_data, output_data)\n",
    "            if q != \"\" and a != \"\":\n",
    "                    questions.append(q)\n",
    "                    answers.append(a)\n",
    "                    contexts.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5945c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(question, generated_questions, embeddings):\n",
    "    \n",
    "    question_vec = np.asarray(embeddings.embed_query(question)).reshape(1, -1)\n",
    "    gen_question_vec = np.asarray(\n",
    "        embeddings.embed_documents(generated_questions)\n",
    "    )\n",
    "    norm = np.linalg.norm(gen_question_vec, axis=1) * np.linalg.norm(\n",
    "        question_vec, axis=1\n",
    "    )\n",
    "    return (\n",
    "        np.dot(gen_question_vec, question_vec.T).reshape(\n",
    "            -1,\n",
    "        )\n",
    "        / norm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6865b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for q, a in zip(questions, answers):\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        results.append(llm_chain.run(answer=a).strip())\n",
    "    cosine_sim = calculate_similarity(q, results, embeddings)\n",
    "    scores.append(cosine_sim.mean())\n",
    "    \n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c8650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827313d6",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734db00d",
   "metadata": {},
   "source": [
    "## Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df252bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTEXT_PRECISION_TEMPLATE = \"\"\"\\n\\nHuman: Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\\nquestion:{question}\n",
    "\\ncontext:\\n{context}\n",
    "\\nAssistant: candidate sentences:\n",
    "\"\"\" \n",
    "\n",
    "EVALUATOR = PromptTemplate(template=CONTEXT_PRECISION_TEMPLATE, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedfdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_overlap(context_sent, generated_context):\n",
    "    overlap_scores = []\n",
    "    for gc in generated_context:\n",
    "        indices = (\n",
    "            sent_tokenize(gc)\n",
    "            if gc.lower() != \"insufficient information.\"\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        if len(context_sent) == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = min(len(indices) / len(context_sent), 1)\n",
    "        \n",
    "        overlap_scores.append(score)\n",
    "            \n",
    "    return np.mean(overlap_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b818b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for q, c in zip(questions, contexts):\n",
    "    if c != \"\":\n",
    "        context_sent = sent_tokenize(c)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(5):\n",
    "            results.append(llm_chain.run(question=q, context=c).strip())\n",
    "\n",
    "        score = calculate_overlap(context_sent, results)\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff49ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3af39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
