{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5f5dfa-b16b-4294-819a-a665c9e55499",
   "metadata": {},
   "source": [
    "# Introduction to SageMaker Jumpstart Built-In Algorithms - Text Embedding\n",
    "> *This notebook should work well with the **`Data Science 2.0`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6631b-2e29-40d3-bed3-9e1fc4105ba4",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to Amazon [SageMaker Jumpstart Built-In Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use Sagemaker Built-In Algorithms to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK for Text Embedding. Text Embedding is the task of converting text into meaningful vector representations. We show how to use pre-trained model to find text embeddings. Additionally, we also show how to leverage these embeddings for Semantic Textual Similarity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febed14-39dd-4719-836b-bf421a1e12a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select a model](#2.-Select-a-model)\n",
    "3. [Retrieve Artifacts & Deploy an Endpoint](#3.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Semantic Textual Similarity](#5.-Semantic-Textual-Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30cd25-3d1c-4013-b4d8-6b7cec8839b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --quiet --upgrade --force-reinstall\n",
    "!pip install ipywidgets==7.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07c2cb-df73-42fc-8662-f7e07e86dfd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5c55a-6bb1-4e1a-9ec2-bcd4cb0d05e9",
   "metadata": {},
   "source": [
    "Show all the Text Embedding Models available to deploy using SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53dfce-5a8f-4f6a-a43c-348ad86a5c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# filter_value = \"task == tcembedding\"\n",
    "filter_value = \"task == textembedding\"\n",
    "# huggingface-textembedding-gpt-j-6b-fp16\n",
    "text_embedding_models = list_jumpstart_models(filter=filter_value)\n",
    "# text_embedding_models = list_jumpstart_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa103dd5-2edd-4115-985f-076738a579f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_embedding_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e2ba7-5041-43b4-a6b3-3de523ec1e55",
   "metadata": {},
   "source": [
    "# What is Vector Embedding\n",
    "## Introduction\n",
    "Vector embeddings are essential components in solving various NLP tasks. It's widely adopted for addressing challenges in providing recommendation, search algorithms and more. \n",
    "\n",
    "In a nutshell, a vector embedding is a numeric representation of an object. These objects could be texts, images, audios or videos. Because they are numerical, we could perform various operations with them. For instance, a whole paragraph of text or any other object can be reduced to a vector. Even numerical data can be turned into vectors for easier operations.\n",
    "\n",
    "The importance of vector embeddings for LLMs extends beyond language generation. These embeddings also facilitate a range of downstream tasks, such as sentiment analysis, named entity recognition, text classification, and more. By incorporating pre-trained vector embeddings, LLMs can leverage the knowledge captured during the embedding training process, leading to improved performance on these tasks.\n",
    "\n",
    "![Vector Embeddings](images/vector-embeddings.png)\n",
    "\n",
    "## Embedding Model Selection\n",
    "There are many embedding models to choose from, ranging from the proprietary models, to open source models. Choosing the right embeddings is an essential step in the development of an LLM application. \n",
    "While important, evaluating the performance of all embedding models is extremely time consuming, in many cases not feasible due to time and hardware resouces required to perform these evaluations. For this, we turn to open source comminity for insights. Hugging Face, a renowned platform in the NLP community hosts the Massive Text Embedding Benchmark (MTEB) Leaderboard. This leaderboard serves as a valuable resource for evaluating the performance of various text embedding models across diverse embedding tasks. For a comprehensive understanding of the MTEB Leaderboard and its significance, I recommend referring to the “MTEB: Massive Text Embedding Benchmark” available at (https://huggingface.co/spaces/mteb). It provides a thorough explanation of the leaderboard’s purpose and insights into different text embedding models. Exploring this resource will broaden your understanding of the text embedding landscape and assist you in making informed decisions for your embedding needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83213e-0922-418b-9e81-f12dafc3c87f",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore an embedding model supported in SageMaker Jumpstart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb224a9-cd20-440b-8627-e38b2ec98377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textembedding-gpt-j-6b-fp16\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa65295-a2e8-4e85-ba16-814a94394846",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffeb19-6cf7-4109-a95f-610e4b5190e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.m5.xlarge\")\n",
    "my_model = JumpStartModel(model_id=model_id)\n",
    "predictor = my_model.deploy() # Provide a unique endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600ba73-0286-46de-9a04-69b537ef859d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Embedding endpoint name: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a473d-de9f-40eb-a16e-50af29051534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query(model_predictor, text):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "\n",
    "    encoded_text = text.encode(\"utf-8\")\n",
    "\n",
    "    query_response = model_predictor.predict(\n",
    "        encoded_text,\n",
    "        {\n",
    "            \"ContentType\": \"application/x-text\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the embedding.\"\"\"\n",
    "\n",
    "    if isinstance(query_response, dict):\n",
    "        model_predictions = query_response\n",
    "    else:\n",
    "        model_predictions = json.loads(query_response)\n",
    "    \n",
    "    translation_text = model_predictions[\"embedding\"]\n",
    "    return translation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26186f-7254-45e0-a895-9fcae7830637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.base_serializers.IdentitySerializer()\n",
    "predictor.deserializer = sagemaker.base_deserializers.BytesDeserializer()\n",
    "embedding_endpoint_name = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abf802-2013-4722-be9e-ac20ed674a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "input_text = \"astonishing ... ( frames ) profound ethical and philosophical questions in the form of dazzling pop entertainment\"\n",
    "\n",
    "\n",
    "query_response = query(predictor, input_text)\n",
    "embedding = parse_response(query_response)\n",
    "\n",
    "print(\n",
    "    f\"{bold}Inference{unbold}:{newline}\"\n",
    "    f\"{bold}Input text sentence{unbold}: '{input_text}'{newline}\"\n",
    "    f\"{bold}The first 5 elements of sentence embedding{unbold}: {embedding[:5]}{newline}\"\n",
    "    f\"{bold}Sentence embedding size{unbold}: {len(embedding)}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e993da-2afe-48f5-b400-0dfff02d34f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c0755-e212-48ec-873c-1b11f8dc2356",
   "metadata": {},
   "source": [
    "A use case of sentence embedding is to cluster together sentences with similar semantic meaning.  In the example below we compute the embeddings of sentences in three categories: pets, cities in the U.S., and color.  We see that sentences originating from the same category have much closer embedding vectors than those from different categories.  Specifically, the code will do the following:\n",
    "The SageMaker embedding model will output an embedding vector for each sentence;  The distance between any pair of sentences is computed by the cosine similarity of corresponded embedding vectors;A heatmap is created to visualize the distance between any pair of sentences in the embedding space. Darker the color, larger the cosine similarity (smaller the distance).\n",
    "Note. Cosine similarity of two vectors is  the inner product of the normalized vectors (scale down to have length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1fba8-d298-4955-9817-973a147f84f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ae1ba-92bc-4489-94f0-304f89cee15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_embedding(predictor, sentence):\n",
    "    query_response = query(predictor, sentence)\n",
    "    embedding = parse_response(query_response)\n",
    "    return embedding\n",
    "    \n",
    "def plot_similarity_heatmap(text_labels, embeddings, rotation):\n",
    "    \"\"\"Takes sentences, embeddings and rotation as input and plot similarity heat map.\n",
    "\n",
    "    Args:\n",
    "      text_labels: a list of sentences to compute semantic textual similarity search.\n",
    "      embeddings: a list of embedding vectors, each of which corresponds to a sentence.\n",
    "      rotation: rotation used for display of the text_labels.\n",
    "    \"\"\"\n",
    "    inner_product = np.inner(embeddings, embeddings)\n",
    "    sns.set(font_scale=1.1)\n",
    "    graph = sns.heatmap(\n",
    "        inner_product,\n",
    "        xticklabels=text_labels,\n",
    "        yticklabels=text_labels,\n",
    "        vmin=np.min(inner_product),\n",
    "        vmax=1,\n",
    "        cmap=\"OrRd\",\n",
    "    )\n",
    "    graph.set_xticklabels(text_labels, rotation=rotation)\n",
    "    graph.set_title(\"Semantic Textual Similarity Between Sentences\")\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    # Pets\n",
    "    \"Your dog is so cute.\",\n",
    "    \"How cute your dog is!\",\n",
    "    \"You have such a cute dog!\",\n",
    "    # Cities in the US\n",
    "    \"New York City is the place where I work.\",\n",
    "    \"I work in New York City.\",\n",
    "    # Color\n",
    "    \"What color do you like the most?\",\n",
    "    \"What is your favourite color?\",\n",
    "]\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    query_response = get_embedding(predictor=predictor, sentence=sentence)\n",
    "    embeddings.append(query_response[0])\n",
    "    \n",
    "embeddings = normalize(np.array(embeddings), axis=1)  # normalization before inner product\n",
    "plot_similarity_heatmap(sentences, embeddings, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c27f6-4850-4d1b-8cdb-e72f45f8f68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store embedding_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609b985-9cdb-48de-8964-af75a50fdf09",
   "metadata": {},
   "source": [
    "### Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f0209-adfc-40c8-a222-e6f35b07921e",
   "metadata": {},
   "source": [
    "In the next lab, we'll develop a knowledge base repository using a vector database that can be used to provide semantic search capability for downstream tasks, including a Q&A bot. We'll use the embedding model to extract the embeddings from the given documents and ingest into the vector database. Additionally, we'll use the embedding model to find similar documents from the vector database to build a RAG based chatbot application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b624c-fbc1-4e21-a391-138b0aa21fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
